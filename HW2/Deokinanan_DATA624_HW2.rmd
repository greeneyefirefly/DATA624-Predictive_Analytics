---
title: 'CUNY MSDS DATA 624 HW #2'
author: "Samantha Deokinanan"
date: "September 13, 2020"
output: 
  html_document:
    theme: cerulean
    highlight: pygments
---

*Textbook: Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. [Link](OTexts.com/fpp2). Accessed on September 7, 2020.*

```{r message=FALSE, warning=FALSE}
# Required R packages
library(fpp2)
library(tidyverse)
library(gridExtra)
```

### Exercise 3.1
For the following series, find an appropriate Box-Cox transformation in order to stabilize the variance.  

* `usnetelec`
* `usgdp`
* `mcopper`
* `enplanements`

```{r}
# Function that finds an appropriate Box-Cox transformation in order to stabilize the variance
bct = function(ts, title){
  a = autoplot(ts) + labs(title = sprintf("Before: %s", title))
  lambda = BoxCox.lambda(ts)
  at = autoplot(BoxCox(ts, lambda)) + labs(title = sprintf("Transformed: %s", title), 
                                           subtitle = sprintf("lambda = %0.2f", lambda))
  grid.arrange(a, at)
}

bct(usnetelec, title = "Annual US Net Electricity Generation")
```

The plot above fits with a transformation of $\lambda = 0.52$, however, there doesn't appear to be any noticeable changes that can help with better interpretation. This may be due to the lack of any trend of increasing or decreasing variation in the time series data. 

```{r}
bct(usgdp, title = "Quarterly US GDP")
```

The plot above fits with a transformation of $\lambda = 0.37$, and it is apparent that the curvature of the graph was reduced.

```{r}
bct(mcopper, title = "Monthly Copper Prices")
```

The plot above fits with a transformation of $\lambda = 0.19$, and the transformation allowed seasonal variation to be more observably stable.

```{r}
bct(enplanements, title = "Monthly US Domestic Enplanements")
```

The plot above fits with a transformation of $\lambda = -0.23$, and this transformation was also able to stabilize the variation in the data.

### Exercise 3.2
Why is a Box-Cox transformation unhelpful for the `cangas` data?

```{r}
bct(cangas, title = "Monthly Canadian Gas Production")
```

After examining the original and transformed plots for the Monthly Canadian Gas Production time series, there is no apparent stabilization of the variation. This may be due to the increasing and decreasing variations of the time series. Note that, from 1960 to 1975 there is an increasing trend, which tapers until 1987 before it increases again. Moreover, there is evidence of seasonality variance in which the middle portion of the data exhibits widely than the lower and upper portions. Therefore, the transformation does not show to be helpful for the plot and it would be helpful if we had a larger transforming $\lambda$ for the lower and upper portion, and a smaller transforming $\lambda$ for the middle portion.

### Exercise 3.3 
What Box-Cox transformation would you select for your retail data (from Exercise 3 in Section 2.10)?

```{r}
retaildata = readxl::read_excel("retail.xlsx", skip = 1)
myts = ts(retaildata[, 13], frequency = 12, start = c(1982,4))
bct(myts, title = "New South Wales - Department Stores")
```

For this time series subset, the optimal Box-Cox transformation is with $\lambda = 0.21$. The resulting transformation highlights the stabilization of the seasonal variability throughout the years. The plot shows how the data before 1990, with smaller variability, were stretched, while after the year 2000, with larger variability, were diminished.

### Exercise 3.8
For your retail time series (from Exercise 3 in Section 2.10): 

a. Split the data into two parts using

```{r}
myts.train = window(myts, end=c(2010,12))
myts.test = window(myts, start=2011)
```

b. Check that your data have been split appropriately by producing the following plot.

```{r}
autoplot(myts) +
autolayer(myts.train, series="Training") +
autolayer(myts.test, series="Test")

# Ensures that data was split appropriately
tail(myts.train)
head(myts.test)
```

c. Calculate forecasts using `snaive` applied to `myts.train`.

```{r}
# Seasonal Naive Method
fc = snaive(myts.train)
fc
```

d. Compare the accuracy of your forecasts against the actual values stored in `myts.test`.

```{r}
accuracy(fc, myts.test)
```

The mean error for the training set is 11.2 while the mean error for the test set is -17.6. Comparing its absolute value, both values are a bit far from zero which usually indicates its not as good of a fit. Next, the root mean square error is very similar for the training and test sets, 24.7 and 25.7 respectively. Likewise, the mean absolute error is very similar, 19.0 and 20.7 respectively. Because the RMSE and MAE are quite similar, this suggests that there is little variance within the errors and that the model does not over-fit.

Moreover, the mean percentage error for the training set is 3.24% and for the testing set, it is 3.86%. The mean absolute percentage error is 5.58% for the training set and 4.50% for the testing test. Lastly, the mean absolute scaled error for the training set is 1.00% and for the testing set, it is 1.08%. 

e. Check the residuals. Do the residuals appear to be uncorrelated and normally distributed?

```{r}
checkresiduals(fc)
```

Here, there are a time plot, ACF plot, and histogram of the residuals (with an overlaid normal distribution for comparison). Firstly, the residual time plot highlights a few large positive and negative residuals after the year 2000 and 2005. Next, the histogram suggests that the residuals may be right-skewed and the mean of the residuals is far from zero. Moreover, there are a few significant correlations in the residuals series, suggesting the forecasts may not be as good. And lastly, based on the Ljung-Box test, the residuals are distinguishable from a white noise series, $Q^* = 98.61, df = 24, p-value < 0.05$. Overall, it seems that forecasts from this method will probably be not as good.

f. How sensitive are the accuracy measures to the training/test split?

To understand the sensitivity of the accuracy measure to the training/test split, the following performs multiple accuracy tests on different splits for comparison.

```{r}
ts_accuracy = function(ts, year){
  train = window(ts, end = c(year, 12))
  test = window(ts, start = year + 1)
  accuracy = data.frame(accuracy(snaive(train), test))
  row.names(accuracy) = c(sprintf("Training Set, end %d", year), sprintf("Test Set, start %d", year + 1))
  return(accuracy)
}

sensitivity_test = data.frame()
for (year in c(2002:2007)){
  test = ts_accuracy(myts, year)
  sensitivity_test = rbind(sensitivity_test, test)
}

sensitivity_test
```

The accuracy test on different splits shows that there are variations in how well the model fits. For instance, the better forecast seems to be if the split was done ending in December 2004 to make a forecast for the following year. The RSME and MAE are 22.5 and 16.9, respectively, and are quite low compared to the values of the other splits. Moreover, the percentage errors are the lowest than the other splits. In conclusion, accuracy measures are very sensitive to the split. 
